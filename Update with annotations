#the 2 codes work independently so we import again and with new packages for the webscraping (pip install also needed)
import requests
import numpy as np
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup

#we connect to our existing DB (use the same file location as last time)
conn = sqlite3.connect("C:/Users/julie/Documents/Databases/NameYourDB.db")

#we create our cursor
cur = conn.cursor()

#getting and storing the html of the page we need on the ECB website
ecb_page = requests.get('https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html') 
list1=[]
list=[]
curr = ['USD', 'THB', 'SEK', 'GBP', 'BRL'] #listing the currencies we are interested in
dateTimeObj = datetime.now() #getting the time of today
dateObj = dateTimeObj.date()
dateStr = dateObj.strftime("%Y-%m-%d") #formating the date to match the format of the DB
new=[]
new_dict = {}
page = BeautifulSoup(ecb_page.content, 'html.parser') #we are going to use BeautifulSoup to parse the html and find the infos we need 

tds = page.findAll('span', class_='rate') #getting the rates of the day
tdp = page.findAll('td', class_='currency') #getting the currencies associated to those rates

for td in tds:
    a = td.text #getting each rate into text
    list.append(a) #adding it to the list

for tr in tdp:
    sr = tr.find('a').text #getting each currency code
    list1.append(sr) #adding it to the list
for i in range(len(list1)):
    new.append(list1[i])
    new.append(list[i])

#creating dict with the currency codes and their rates
new_dict["Name"] = list1
new_dict["Value"] = list

dr1 = pd.DataFrame(new_dict) #creating pandas dataframe using that dict

#now that we have retrieves the information we need and structured it in a way we can use it
#Let's define our insert function
def insert_webscr(date_wb, curr_wb, rate):
    cur.execute('''INSERT OR REPLACE into transactions(date, currCode, actualRate, source) 
                    VALUES( ? , ? , ? , 'Webscraping');''', (date_wb, curr_wb, rate)) #we need to specify the variables at the end
                    
    conn.commit() #hitting 'save'

#same function as before
def insert_forecast(a, date, currCode, arr_mean):
    mean = np.mean(a['actualRate'])
    if (a.size >= arr_mean) :
        update_sql = "update transactions set forecastedRate = " + str(mean) + " where date = '" + date + "' and currCode = '" + currCode + "'"
        cur.execute(update_sql)
        conn.commit()

#same function as before except we specify the date to apply it to
def update_error_wb():
    cur.execute("select * from transactions")
    data = cur.fetchall()
    for x in data:
        list = [x[0],x[1],x[2],x[3],x[4],x[5]]
        if x[0] == dateStr: #specifying the date
            try:
                error = np.subtract(list[2],list[4])
                cur.execute("UPDATE transactions SET forecastError = " + str(error) + " where date = '" + list[0] + "' and currCode = '" + list[1] + "'")
                conn.commit()
            except:
                pass

#loop that inserts the webscraping into the DB
for index,row in dr1.iterrows():
    if row['Name'] in curr: #but only if it is one of our chosen currencies
        insert_webscr(dateStr, row['Name'], row['Value'])
        
#we will need to reset our index again 
#therefore we repeat the steps
query = "select * from transactions order by date;"
test = pd.read_sql_query(query, conn)
        
arr_mean = 3
for index, row in test.iterrows() :
    t = test[test['currCode']==row['currCode']]
    t.reset_index(drop=True, inplace=True)
    i = t[(t['date']==row['date']) & (t['currCode']==row['currCode'])].index[0]
    if row['date'] == dateStr: #only difference is here because we do not want to recalculate the forecast for each row
        insert_forecast(t[i-arr_mean:i], row['date'], row['currCode'], arr_mean)
    
update_error_wb() #then we add the new error
